{
  "family": "gameforge-sdxl-gpu-us-west-2",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["EC2"],
  "cpu": "4096",
  "memory": "16384",
  "executionRoleArn": "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
  "taskRoleArn": "arn:aws:iam::927588814706:role/ecsTaskRole",
  "placementConstraints": [
    {
      "type": "memberOf",
      "expression": "attribute:ecs.instance-type =~ g5.*"
    }
  ],
  "containerDefinitions": [
    {
      "name": "sdxl-gpu-service-us-west-2",
      "image": "pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel",
      "essential": true,
      "resourceRequirements": [
        {
          "type": "GPU",
          "value": "1"
        }
      ],
      "portMappings": [
        {
          "containerPort": 8080,
          "protocol": "tcp",
          "name": "sdxl-gpu-port"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/aws/ecs/gameforge-sdxl-gpu",
          "awslogs-region": "us-west-2",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "environment": [
        {
          "name": "PORT",
          "value": "8080"
        },
        {
          "name": "PYTHON_ENV",
          "value": "production"
        },
        {
          "name": "AWS_REGION",
          "value": "us-west-2"
        },
        {
          "name": "CUDA_VISIBLE_DEVICES",
          "value": "0"
        },
        {
          "name": "TORCH_CACHE",
          "value": "/tmp/torch_cache"
        },
        {
          "name": "TRANSFORMERS_CACHE",
          "value": "/tmp/transformers_cache"
        },
        {
          "name": "HF_HOME",
          "value": "/tmp/hf_cache"
        },
        {
          "name": "PYTORCH_CUDA_ALLOC_CONF",
          "value": "max_split_size_mb:512"
        },
        {
          "name": "NVIDIA_VISIBLE_DEVICES",
          "value": "all"
        },
        {
          "name": "NVIDIA_DRIVER_CAPABILITIES",
          "value": "compute,utility"
        },
        {
          "name": "GPU_TYPE",
          "value": "A10G"
        },
        {
          "name": "REGION_OPTIMIZED",
          "value": "us-west-2"
        }
      ],
      "mountPoints": [
        {
          "sourceVolume": "gpu_cache",
          "containerPath": "/tmp/torch_cache",
          "readOnly": false
        },
        {
          "sourceVolume": "model_cache",
          "containerPath": "/tmp/models",
          "readOnly": false
        }
      ],
      "command": [
        "/bin/bash",
        "-c",
        "echo 'Starting GameForge SDXL GPU Service in us-west-2' && echo 'GPU Type: A10G' && nvidia-smi && pip install --no-cache-dir fastapi uvicorn[standard] torch torchvision --index-url https://download.pytorch.org/whl/cu121 && pip install --no-cache-dir diffusers transformers accelerate xformers && python -c \"import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA device count: {torch.cuda.device_count()}'); print(f'Current device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"}')\"; cat > /app/main.py << 'EOF'\nimport torch\nimport time\nimport os\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom diffusers import StableDiffusionXLPipeline\nfrom typing import Optional\nimport base64\nimport io\nfrom PIL import Image\n\napp = FastAPI(title=\"GameForge SDXL GPU Service\", version=\"2.0.0\")\n\n# Global variables for model caching\nMODEL_CACHE = None\nGPU_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nREGION = os.getenv(\"AWS_REGION\", \"us-west-2\")\nGPU_TYPE = os.getenv(\"GPU_TYPE\", \"A10G\")\n\nclass ImageRequest(BaseModel):\n    prompt: str\n    negative_prompt: Optional[str] = \"\"\n    width: int = 1024\n    height: int = 1024\n    num_inference_steps: int = 20\n    guidance_scale: float = 7.5\n    seed: Optional[int] = None\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    global MODEL_CACHE\n    print(f\"ðŸš€ Starting GameForge SDXL GPU Service\")\n    print(f\"ðŸ“ Region: {REGION}\")\n    print(f\"ðŸ–¥ï¸  GPU Type: {GPU_TYPE}\")\n    print(f\"ðŸ”§ Device: {GPU_DEVICE}\")\n    \n    if torch.cuda.is_available():\n        print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n        print(f\"ðŸ§  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n    \n    try:\n        print(\"ðŸ“¦ Loading SDXL model...\")\n        MODEL_CACHE = StableDiffusionXLPipeline.from_pretrained(\n            \"segmind/SSD-1B\",\n            torch_dtype=torch.float16,\n            use_safetensors=True\n        ).to(GPU_DEVICE)\n        \n        if GPU_DEVICE == \"cuda\":\n            MODEL_CACHE.enable_xformers_memory_efficient_attention()\n            MODEL_CACHE.enable_model_cpu_offload()\n        \n        print(f\"âœ… Model loaded successfully on {GPU_DEVICE}\")\n    except Exception as e:\n        print(f\"âŒ Model loading failed: {e}\")\n        MODEL_CACHE = None\n\n@app.get(\"/health\")\nasync def health_check():\n    gpu_info = {}\n    if torch.cuda.is_available():\n        gpu_info = {\n            \"gpu_name\": torch.cuda.get_device_name(0),\n            \"gpu_memory_allocated\": torch.cuda.memory_allocated(0),\n            \"gpu_memory_cached\": torch.cuda.memory_reserved(0)\n        }\n    \n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": MODEL_CACHE is not None,\n        \"device\": GPU_DEVICE,\n        \"region\": REGION,\n        \"gpu_type\": GPU_TYPE,\n        \"gpu_info\": gpu_info,\n        \"pytorch_version\": torch.__version__\n    }\n\n@app.post(\"/generate\")\nasync def generate_image(request: ImageRequest):\n    if MODEL_CACHE is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n    \n    try:\n        start_time = time.time()\n        \n        # Set seed for reproducibility\n        if request.seed:\n            torch.manual_seed(request.seed)\n        \n        # Generate image with GPU acceleration\n        with torch.inference_mode():\n            image = MODEL_CACHE(\n                prompt=request.prompt,\n                negative_prompt=request.negative_prompt,\n                width=request.width,\n                height=request.height,\n                num_inference_steps=request.num_inference_steps,\n                guidance_scale=request.guidance_scale\n            ).images[0]\n        \n        generation_time = time.time() - start_time\n        \n        # Convert to base64\n        buffered = io.BytesIO()\n        image.save(buffered, format=\"PNG\")\n        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n        \n        return {\n            \"image\": img_base64,\n            \"generation_time\": round(generation_time, 2),\n            \"device\": GPU_DEVICE,\n            \"region\": REGION,\n            \"gpu_type\": GPU_TYPE,\n            \"prompt\": request.prompt,\n            \"dimensions\": f\"{request.width}x{request.height}\"\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"service\": \"GameForge SDXL GPU Service\",\n        \"version\": \"2.0.0\",\n        \"region\": REGION,\n        \"gpu_type\": GPU_TYPE,\n        \"device\": GPU_DEVICE,\n        \"endpoints\": [\"/health\", \"/generate\"]\n    }\n\nEOF\necho 'ðŸš€ Starting FastAPI server on port 8080...' && cd /app && python -m uvicorn main:app --host 0.0.0.0 --port 8080"
      ],
      "healthCheck": {
        "command": [
          "CMD-SHELL",
          "curl -f http://localhost:8080/health || exit 1"
        ],
        "interval": 30,
        "timeout": 5,
        "retries": 3,
        "startPeriod": 120
      }
    }
  ],
  "volumes": [
    {
      "name": "gpu_cache",
      "host": {
        "sourcePath": "/tmp/gpu_cache"
      }
    },
    {
      "name": "model_cache",
      "host": {
        "sourcePath": "/tmp/model_cache"
      }
    }
  ],
  "tags": [
    {
      "key": "Service",
      "value": "GameForge-SDXL-GPU"
    },
    {
      "key": "Region",
      "value": "us-west-2"
    },
    {
      "key": "GPU-Type",
      "value": "A10G"
    },
    {
      "key": "Environment",
      "value": "Production"
    }
  ]
}
